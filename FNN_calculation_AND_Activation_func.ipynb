{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s break down the computation in a feedforward neural network (FNN) using a standard activation function and then see how that differs when using a Gated Linear Unit (GLU).\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Standard FNN Calculation with an Activation Function\n",
    "\n",
    "In a typical FNN layer, you perform a linear transformation followed by a non-linear activation. Here’s the step-by-step calculation:\n",
    "\n",
    "1. **Linear Transformation:**\n",
    "\n",
    "   Given an input vector \\(\\mathbf{x} \\in \\mathbb{R}^n\\), a weight matrix \\(\\mathbf{W} \\in \\mathbb{R}^{m \\times n}\\), and a bias vector \\(\\mathbf{b} \\in \\mathbb{R}^{m}\\), the linear part computes:\n",
    "   \\[\n",
    "   \\mathbf{z} = \\mathbf{W}\\mathbf{x} + \\mathbf{b}.\n",
    "   \\]\n",
    "\n",
    "2. **Non-linear Activation:**\n",
    "\n",
    "   An activation function \\(f\\) (such as ReLU, tanh, or GELU) is then applied element-wise:\n",
    "   \\[\n",
    "   \\mathbf{a} = f(\\mathbf{z}).\n",
    "   \\]\n",
    "   For instance, with ReLU:\n",
    "   \\[\n",
    "   \\text{ReLU}(z) = \\max(0, z).\n",
    "   \\]\n",
    "\n",
    "This two-step process allows the network to model complex, non-linear relationships.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. FNN Calculation with a Gated Linear Unit (GLU)\n",
    "\n",
    "The Gated Linear Unit introduces a gating mechanism that modulates the output of a linear transformation. Instead of applying a single activation function directly to the linear output, GLU splits the linear output into two parts and uses one part to control the flow of the other.\n",
    "\n",
    "### **Calculation Steps for GLU**\n",
    "\n",
    "1. **Linear Transformation with Doubling of Channels:**\n",
    "\n",
    "   Instead of computing one set of outputs, the network computes a combined output that is twice as large. Suppose the input \\(\\mathbf{x} \\in \\mathbb{R}^n\\) and you want an output of dimension \\(m\\). You compute:\n",
    "   \\[\n",
    "   \\mathbf{z} = \\mathbf{x} \\mathbf{W} + \\mathbf{b},\n",
    "   \\]\n",
    "   where now \\(\\mathbf{W} \\in \\mathbb{R}^{n \\times 2m}\\) and \\(\\mathbf{b} \\in \\mathbb{R}^{2m}\\).\n",
    "\n",
    "2. **Splitting the Output:**\n",
    "\n",
    "   The result \\(\\mathbf{z}\\) is split into two vectors:\n",
    "   \\[\n",
    "   \\mathbf{z} = \\left[ \\mathbf{z}_A \\; \\mathbf{z}_B \\right],\n",
    "   \\]\n",
    "   where \\(\\mathbf{z}_A, \\mathbf{z}_B \\in \\mathbb{R}^{m}\\).\n",
    "\n",
    "3. **Gating Mechanism:**\n",
    "\n",
    "   A gating function, typically the sigmoid \\(\\sigma\\), is applied to \\(\\mathbf{z}_B\\) to produce a gate vector:\n",
    "   \\[\n",
    "   \\mathbf{g} = \\sigma(\\mathbf{z}_B),\n",
    "   \\]\n",
    "   where\n",
    "   \\[\n",
    "   \\sigma(z) = \\frac{1}{1 + e^{-z}}.\n",
    "   \\]\n",
    "\n",
    "4. **Element-wise Multiplication (Gated Output):**\n",
    "\n",
    "   The final output of the GLU is the element-wise multiplication of \\(\\mathbf{z}_A\\) and the gate \\(\\mathbf{g}\\):\n",
    "   \\[\n",
    "   \\text{GLU}(\\mathbf{x}) = \\mathbf{z}_A \\odot \\mathbf{g}.\n",
    "   \\]\n",
    "\n",
    "This mechanism lets the network learn to “gate” or control how much of the linear transformation’s output should pass through, providing a more flexible and dynamic non-linearity.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Comparison and Pros & Cons\n",
    "\n",
    "### **Standard Activation in FNNs**\n",
    "\n",
    "- **Pros:**\n",
    "  - **Simplicity:** The computation is straightforward—just a linear transformation followed by an activation.\n",
    "  - **Efficiency:** Fewer parameters and lower computational overhead.\n",
    "  - **Effective in Many Settings:** Works well for many tasks when the network is not extremely deep or when simple non-linearity is sufficient.\n",
    "\n",
    "- **Cons:**\n",
    "  - **Limited Expressiveness:** A single fixed non-linearity might not capture complex interactions as flexibly as gating mechanisms.\n",
    "\n",
    "### **GLU-Based FNNs**\n",
    "\n",
    "- **Pros:**\n",
    "  - **Adaptive Gating:** The gate can learn to control the flow of information, potentially filtering out noise or emphasizing important features.\n",
    "  - **Enhanced Expressiveness:** By modulating the linear output, GLU can model more complex interactions and dependencies.\n",
    "  - **Empirical Success:** GLUs have been used effectively in several architectures (e.g., gated convolutional networks in language modeling) to improve performance.\n",
    "\n",
    "- **Cons:**\n",
    "  - **Increased Parameter Count:** Since the linear transformation outputs twice as many features, the number of parameters is roughly doubled.\n",
    "  - **Higher Computational Cost:** More computations are required due to the gating mechanism and the extra parameters.\n",
    "  - **Complexity in Tuning:** The dynamics of the gating function might require careful tuning in certain architectures or applications.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary Equation Comparison\n",
    "\n",
    "- **Standard FNN Layer:**\n",
    "  \\[\n",
    "  \\mathbf{a} = f\\bigl(\\mathbf{W}\\mathbf{x} + \\mathbf{b}\\bigr).\n",
    "  \\]\n",
    "\n",
    "- **GLU FNN Layer:**\n",
    "  \\[\n",
    "  \\begin{aligned}\n",
    "  \\mathbf{z} &= \\mathbf{x}\\mathbf{W} + \\mathbf{b} \\quad \\text{(with } \\mathbf{W} \\in \\mathbb{R}^{n \\times 2m} \\text{)}, \\\\\n",
    "  \\mathbf{z} &= [\\mathbf{z}_A, \\mathbf{z}_B], \\\\\n",
    "  \\text{GLU}(\\mathbf{x}) &= \\mathbf{z}_A \\odot \\sigma(\\mathbf{z}_B).\n",
    "  \\end{aligned}\n",
    "  \\]\n",
    "\n",
    "In summary, while standard activation functions provide a simple and effective non-linearity for feedforward neural networks, GLUs offer an additional level of control through a gating mechanism, which can enhance model expressiveness at the cost of increased complexity and computational overhead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are nine commonly used activation functions along with their mathematical formulations, advantages, and disadvantages.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Sigmoid\n",
    "\n",
    "- **Math:**  \n",
    "  \\[\n",
    "  \\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
    "  \\]\n",
    "\n",
    "- **Pros:**  \n",
    "  - Smooth and differentiable.  \n",
    "  - Outputs in the range (0, 1), which is useful for probabilistic interpretations (e.g., binary classification).\n",
    "\n",
    "- **Cons:**  \n",
    "  - Prone to saturation for very high or low values, leading to vanishing gradients.  \n",
    "  - Not zero-centered, which can slow down convergence in some networks.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Tanh (Hyperbolic Tangent)\n",
    "\n",
    "- **Math:**  \n",
    "  \\[\n",
    "  \\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\n",
    "  \\]\n",
    "\n",
    "- **Pros:**  \n",
    "  - Zero-centered output, which can aid optimization.  \n",
    "  - Steeper gradient than sigmoid around 0.\n",
    "\n",
    "- **Cons:**  \n",
    "  - Also saturates for large positive or negative values, causing vanishing gradients in deep networks.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. ReLU (Rectified Linear Unit)\n",
    "\n",
    "- **Math:**  \n",
    "  \\[\n",
    "  f(x) = \\max(0, x)\n",
    "  \\]\n",
    "\n",
    "- **Pros:**  \n",
    "  - Simple and computationally efficient.  \n",
    "  - Does not saturate in the positive region, which helps mitigate vanishing gradients.\n",
    "\n",
    "- **Cons:**  \n",
    "  - Can lead to “dying ReLU” where neurons become inactive if they consistently output zero.  \n",
    "  - Not differentiable at \\(x = 0\\) (though this is typically not a problem in practice).\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Leaky ReLU\n",
    "\n",
    "- **Math:**  \n",
    "  \\[\n",
    "  f(x) = \\begin{cases}\n",
    "  x, & x \\ge 0 \\\\\n",
    "  \\alpha x, & x < 0\n",
    "  \\end{cases}\n",
    "  \\]\n",
    "  where \\(\\alpha\\) is a small constant (e.g., 0.01).\n",
    "\n",
    "- **Pros:**  \n",
    "  - Mitigates the dying ReLU problem by allowing a small, non-zero gradient for \\(x < 0\\).\n",
    "\n",
    "- **Cons:**  \n",
    "  - Introduces an extra hyperparameter (\\(\\alpha\\)) that needs tuning.  \n",
    "  - Still may not fully capture the best behavior for negative activations in all cases.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. PReLU (Parametric ReLU)\n",
    "\n",
    "- **Math:**  \n",
    "  \\[\n",
    "  f(x) = \\begin{cases}\n",
    "  x, & x \\ge 0 \\\\\n",
    "  \\alpha x, & x < 0\n",
    "  \\end{cases}\n",
    "  \\]\n",
    "  Here, \\(\\alpha\\) is learned during training rather than fixed.\n",
    "\n",
    "- **Pros:**  \n",
    "  - Adaptively learns the slope for negative inputs, potentially improving performance.  \n",
    "  - Can alleviate the dying ReLU problem more flexibly than Leaky ReLU.\n",
    "\n",
    "- **Cons:**  \n",
    "  - Increases the number of parameters, potentially leading to overfitting if not regularized properly.  \n",
    "  - Adds extra computational complexity.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. ELU (Exponential Linear Unit)\n",
    "\n",
    "- **Math:**  \n",
    "  \\[\n",
    "  f(x) = \\begin{cases}\n",
    "  x, & x \\ge 0 \\\\\n",
    "  \\alpha (e^x - 1), & x < 0\n",
    "  \\end{cases}\n",
    "  \\]\n",
    "  where \\(\\alpha\\) is a hyperparameter (commonly set to 1).\n",
    "\n",
    "- **Pros:**  \n",
    "  - Provides a smooth transition for negative inputs, which can improve learning.  \n",
    "  - Often converges faster and leads to higher classification accuracy in some tasks.\n",
    "\n",
    "- **Cons:**  \n",
    "  - Computationally more expensive than ReLU due to the exponential function.  \n",
    "  - The hyperparameter \\(\\alpha\\) must be set appropriately.\n",
    "\n",
    "---\n",
    "\n",
    "### 7. SELU (Scaled Exponential Linear Unit)\n",
    "\n",
    "- **Math:**  \n",
    "  \\[\n",
    "  f(x) = \\lambda \\begin{cases}\n",
    "  x, & x \\ge 0 \\\\\n",
    "  \\alpha (e^x - 1), & x < 0\n",
    "  \\end{cases}\n",
    "  \\]\n",
    "  with fixed parameters, typically \\(\\alpha \\approx 1.67326\\) and \\(\\lambda \\approx 1.0507\\).\n",
    "\n",
    "- **Pros:**  \n",
    "  - Designed for self-normalizing neural networks, helping to keep activations within a desired range automatically.  \n",
    "  - Can improve convergence speed in deep networks.\n",
    "\n",
    "- **Cons:**  \n",
    "  - Sensitive to network architecture choices (e.g., weight initialization, dropout).  \n",
    "  - Not as broadly adopted, so fewer “out-of-the-box” guidelines exist compared to ReLU.\n",
    "\n",
    "---\n",
    "\n",
    "### 8. GELU (Gaussian Error Linear Unit)\n",
    "\n",
    "- **Math:**  \n",
    "  A common approximation is:\n",
    "  \\[\n",
    "  \\text{GELU}(x) \\approx 0.5x \\left(1 + \\tanh\\left[\\sqrt{\\frac{2}{\\pi}}\\left(x + 0.044715x^3\\right)\\right]\\right)\n",
    "  \\]\n",
    "  The exact formulation is:\n",
    "  \\[\n",
    "  \\text{GELU}(x) = x \\cdot \\Phi(x)\n",
    "  \\]\n",
    "  where \\(\\Phi(x)\\) is the cumulative distribution function of the standard normal distribution.\n",
    "\n",
    "- **Pros:**  \n",
    "  - Provides a smooth, non-linear activation that often improves performance, especially in transformer-based architectures.  \n",
    "  - Can capture more nuanced behaviors than ReLU.\n",
    "\n",
    "- **Cons:**  \n",
    "  - More computationally expensive than simpler activations like ReLU.  \n",
    "  - The formulation is more complex and may require careful implementation for efficiency.\n",
    "\n",
    "---\n",
    "\n",
    "### 9. Swish\n",
    "\n",
    "- **Math:**  \n",
    "  \\[\n",
    "  f(x) = x \\cdot \\sigma(\\beta x)\n",
    "  \\]\n",
    "  where \\(\\sigma(x)\\) is the sigmoid function and \\(\\beta\\) is either a constant (often set to 1) or a learnable parameter.\n",
    "\n",
    "- **Pros:**  \n",
    "  - Smooth and non-monotonic, which can lead to better performance in some deep networks.  \n",
    "  - Has been shown empirically to outperform ReLU on certain tasks.\n",
    "\n",
    "- **Cons:**  \n",
    "  - Computationally more complex due to the additional sigmoid computation.  \n",
    "  - Not as widely standardized in all frameworks, so hardware support might be limited compared to simpler functions.\n",
    "\n",
    "---\n",
    "\n",
    "Each of these activations has its own niche. Choosing the right one often depends on your network’s depth, the nature of your data, and empirical performance on your specific task."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
