{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A feedforward neural network is a type of artificial neural network where the information moves strictly in one direction—from the input layer, through one or more hidden layers, to the output layer. There are no cycles or loops, meaning each neuron receives input from the previous layer and passes its output only to the next layer. This architecture is often referred to as a Multi-Layer Perceptron (MLP) when it contains one or more hidden layers.\n",
    "\n",
    "Below is a detailed breakdown, including the essential math:\n",
    "\n",
    "---\n",
    "\n",
    "### 1. **Architecture Overview**\n",
    "\n",
    "- **Input Layer:**  \n",
    "  The network receives the input data as a vector. For example, if your data has \\( n \\) features, then the input is represented as  \n",
    "  \\[\n",
    "  \\mathbf{x} = [x_1, x_2, \\dots, x_n]^T \\in \\mathbb{R}^n.\n",
    "  \\]\n",
    "\n",
    "- **Hidden Layers:**  \n",
    "  One or more layers where neurons process the input using weighted sums followed by an activation function. Each hidden layer transforms its input into a higher-level representation.\n",
    "\n",
    "- **Output Layer:**  \n",
    "  This layer produces the final output of the network. The nature of the output (e.g., probabilities for classification, continuous values for regression) determines the choice of activation function in this layer.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Mathematical Formulation of a Single Layer**\n",
    "\n",
    "Consider a layer with an input vector \\(\\mathbf{x}\\), weight matrix \\(\\mathbf{W}\\), bias vector \\(\\mathbf{b}\\), and activation function \\(f\\). The computations in this layer are:\n",
    "\n",
    "1. **Linear Transformation (Weighted Sum):**\n",
    "   \\[\n",
    "   \\mathbf{z} = \\mathbf{W}\\mathbf{x} + \\mathbf{b},\n",
    "   \\]\n",
    "   where:\n",
    "   - \\(\\mathbf{W} \\in \\mathbb{R}^{m \\times n}\\) if the layer has \\(m\\) neurons,\n",
    "   - \\(\\mathbf{b} \\in \\mathbb{R}^{m}\\).\n",
    "\n",
    "2. **Activation:**\n",
    "   \\[\n",
    "   \\mathbf{a} = f(\\mathbf{z}).\n",
    "   \\]\n",
    "   The function \\(f\\) could be a non-linear activation such as:\n",
    "   - **Sigmoid:** \\(\\sigma(z) = \\frac{1}{1 + e^{-z}}\\),\n",
    "   - **Hyperbolic Tangent (tanh):** \\(\\tanh(z) = \\frac{e^z - e^{-z}}{e^z + e^{-z}}\\),\n",
    "   - **ReLU:** \\(\\text{ReLU}(z) = \\max(0, z)\\).\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Stacking Layers: The Complete Feedforward Process**\n",
    "\n",
    "In a network with \\(L\\) layers (including the output layer), the forward pass through the network can be written as:\n",
    "\n",
    "- **First Hidden Layer:**\n",
    "  \\[\n",
    "  \\mathbf{a}^{(1)} = f^{(1)}\\left(\\mathbf{W}^{(1)} \\mathbf{x} + \\mathbf{b}^{(1)}\\right)\n",
    "  \\]\n",
    "\n",
    "- **Subsequent Hidden Layers:**\n",
    "  For layer \\( l \\) (where \\( 2 \\leq l \\leq L-1 \\)):\n",
    "  \\[\n",
    "  \\mathbf{a}^{(l)} = f^{(l)}\\left(\\mathbf{W}^{(l)} \\mathbf{a}^{(l-1)} + \\mathbf{b}^{(l)}\\right)\n",
    "  \\]\n",
    "\n",
    "- **Output Layer:**\n",
    "  \\[\n",
    "  \\mathbf{y} = f^{(L)}\\left(\\mathbf{W}^{(L)} \\mathbf{a}^{(L-1)} + \\mathbf{b}^{(L)}\\right)\n",
    "  \\]\n",
    "\n",
    "Here, each \\( \\mathbf{W}^{(l)} \\) and \\( \\mathbf{b}^{(l)} \\) are the weight matrix and bias vector for layer \\( l \\), and each \\( f^{(l)} \\) is the activation function applied in that layer.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Training the Network: Backpropagation and Gradient Descent**\n",
    "\n",
    "The goal during training is to adjust the weights and biases so that the network's output \\(\\mathbf{y}\\) is as close as possible to the desired target \\(\\mathbf{t}\\). This is typically done by minimizing a loss (or cost) function \\(J\\).\n",
    "\n",
    "- **Example Loss Functions:**\n",
    "  - **Mean Squared Error (MSE):**\n",
    "    \\[\n",
    "    J(\\mathbf{y}, \\mathbf{t}) = \\frac{1}{2} \\sum_{i} (y_i - t_i)^2,\n",
    "    \\]\n",
    "  - **Cross-Entropy Loss (for classification):**  \n",
    "    (Depends on the specific formulation, often paired with a softmax activation in the output layer.)\n",
    "\n",
    "- **Backpropagation:**\n",
    "  This algorithm computes the gradient of the loss function with respect to each weight and bias using the chain rule. The key steps are:\n",
    "\n",
    "  1. **Compute the error at the output layer:**\n",
    "     \\[\n",
    "     \\delta^{(L)} = \\nabla_{\\mathbf{a}^{(L)}} J \\odot f'^{(L)}(\\mathbf{z}^{(L)}),\n",
    "     \\]\n",
    "     where \\(\\odot\\) denotes the element-wise (Hadamard) product and \\( f'^{(L)} \\) is the derivative of the activation function at the output layer.\n",
    "\n",
    "  2. **Propagate the error backward for hidden layers:**\n",
    "     For \\( l = L-1, L-2, \\dots, 1 \\):\n",
    "     \\[\n",
    "     \\delta^{(l)} = \\left( (\\mathbf{W}^{(l+1)})^T \\delta^{(l+1)} \\right) \\odot f'^{(l)}(\\mathbf{z}^{(l)}).\n",
    "     \\]\n",
    "\n",
    "  3. **Gradient Descent Update:**\n",
    "     The weights and biases are updated using a learning rate \\(\\eta\\):\n",
    "     \\[\n",
    "     \\mathbf{W}^{(l)} \\leftarrow \\mathbf{W}^{(l)} - \\eta \\, \\frac{\\partial J}{\\partial \\mathbf{W}^{(l)}},\n",
    "     \\]\n",
    "     \\[\n",
    "     \\mathbf{b}^{(l)} \\leftarrow \\mathbf{b}^{(l)} - \\eta \\, \\frac{\\partial J}{\\partial \\mathbf{b}^{(l)}}.\n",
    "     \\]\n",
    "  \n",
    "  This iterative process of forward pass, loss computation, backpropagation, and parameter update continues until the network’s performance on the training data is satisfactory.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **Summary**\n",
    "\n",
    "- **Feedforward Neural Network:**  \n",
    "  A network where data flows only in one direction—from input to output—with no cycles or loops.\n",
    "\n",
    "- **Core Computation:**  \n",
    "  Each layer computes a weighted sum of its inputs, adds a bias, and then applies a non-linear activation function.\n",
    "\n",
    "- **Mathematics:**  \n",
    "  The operation for each layer can be expressed as:\n",
    "  \\[\n",
    "  \\mathbf{a} = f(\\mathbf{W}\\mathbf{x} + \\mathbf{b}),\n",
    "  \\]\n",
    "  and in networks with multiple layers, this process is repeated sequentially.\n",
    "\n",
    "- **Training:**  \n",
    "  The network is trained using backpropagation to compute gradients and update the parameters via gradient descent, minimizing a loss function that quantifies the difference between the network's predictions and the actual targets.\n",
    "\n",
    "This detailed overview should provide a comprehensive understanding of feedforward neural networks, including the mathematical foundations underlying their operation."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
